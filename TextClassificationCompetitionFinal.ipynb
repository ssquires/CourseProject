{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TextClassificationCompetitionFinal.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOk56mb7aDaTBs3HDsrs7Wb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"fRTPN3lMIPyo"},"source":["# Classification Competition: Twitter Sarcasm Detection with BERT\r\n","\r\n","## Instructions\r\n","To run the full code, please follow these steps:\r\n","\r\n","1. In the Colab \"Edit\" menu above, go to \"Notebook Settings\" and select \"GPU\" from the hardware accelerator dropdown.\r\n","\r\n","2. Upload the train and test data files provided with the competition (make sure they're named `train.jsonl` and `test.jsonl`) by going to \"Files\" in the left-hand sidebar, clicking the upload icon, and selecting `train.jsonl` and `test.jsonl`. \r\n","\r\n","3. To run the code, select the \"Runtime\" menu at the top and click \"Run all\". \r\n","\r\n","## Runtime and Output\r\n","\r\n","The first few cells should run quite quickly: the final cell, which trains the model and predicts labels for the test data, takes much longer (in my experience, 10-12 minutes). After a minute or so, you should start to see output tracking the training progress of the model.\r\n","\r\n","After the code finishes running, the output file, `answer.txt`, should be visible under \"Files\" in the left-hand sidebar. If you'd like to save this file, make sure to download it before the runtime disconnects.\r\n","\r\n","## Note on Non-Determinism\r\n","\r\n","It appears that, despite setting random seeds and using the same train/test split every time, the BERT model is non-deterministic. Each generated `answer.txt` is slightly different, and I'm not positive what percentage of the time they pass the baseline. Out of 5 consecutive attempts generating `answer.txt` and submitting to LiveDataLab today, my resulting F1 scores were:\r\n","\r\n","* 0.7315 (passing baseline)\r\n","* 0.7274 (passing baseline)\r\n","* 0.7326 (passing baseline)\r\n","* 0.7111 (NOT passing baseline)\r\n","* 0.7327 (passing baseline)\r\n","\r\n","Overall, it appears that the model usually, but not always, passes the baseline.\r\n","\r\n","## References\r\n","\r\n","To complete this project, I found Tensorflow's tutorial [\"Classify Text with BERT\"](https://www.tensorflow.org/tutorials/text/classify_text_with_bert) extremely helpful, and directly used some of the tutorial's code. This is also noted in the comments of the relevant functions below.  "]},{"cell_type":"markdown","metadata":{"id":"DSEfPhLLLCDX"},"source":["### 1. Install `tensorflow_text` and `tf-models-official` dependencies"]},{"cell_type":"code","metadata":{"id":"sOyHL4bCesve","executionInfo":{"status":"ok","timestamp":1607822259161,"user_tz":480,"elapsed":4066,"user":{"displayName":"Sam Squires","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqEAf4ZjxyoXjCT4o-1d19bfq9In0PRa0MDNcW__I=s64","userId":"16893370824620180660"}}},"source":["%%capture\r\n","!pip install tensorflow_text"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"h25yaWrme1F1","executionInfo":{"status":"ok","timestamp":1607822269504,"user_tz":480,"elapsed":14402,"user":{"displayName":"Sam Squires","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqEAf4ZjxyoXjCT4o-1d19bfq9In0PRa0MDNcW__I=s64","userId":"16893370824620180660"}}},"source":["%%capture\r\n","!pip install tf-models-official"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xH4AEoZ2LJ-1"},"source":["### 2. Import required packages"]},{"cell_type":"code","metadata":{"id":"4KtsJvlEHM_r","executionInfo":{"status":"ok","timestamp":1607822272002,"user_tz":480,"elapsed":16898,"user":{"displayName":"Sam Squires","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqEAf4ZjxyoXjCT4o-1d19bfq9In0PRa0MDNcW__I=s64","userId":"16893370824620180660"}}},"source":["import numpy as np\r\n","from official.nlp import optimization\r\n","import pandas as pd\r\n","import random\r\n","import tensorflow as tf\r\n","import tensorflow_hub as hub\r\n","import tensorflow_text\r\n","from sklearn.model_selection import train_test_split"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ajYBzBKuLPXA"},"source":["### 3. Define functions for creating the Tensorflow model, training/making predictions, and creating the final output file"]},{"cell_type":"code","metadata":{"id":"-7_rj_AIe2UU","executionInfo":{"status":"ok","timestamp":1607822272003,"user_tz":480,"elapsed":16897,"user":{"displayName":"Sam Squires","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqEAf4ZjxyoXjCT4o-1d19bfq9In0PRa0MDNcW__I=s64","userId":"16893370824620180660"}}},"source":["def get_bert_classifier():\r\n","    \"\"\"Builds the classifier model we'll use to classify tweets as SARCASM or \r\n","       NOT_SARCASM.\r\n","\r\n","       SOURCE: This function borrows heavily from Tensorflow's \"Classify Text \r\n","       with BERT\" tutorial at the URL below: \r\n","       https://www.tensorflow.org/tutorials/text/classify_text_with_bert. \r\n","    \"\"\"\r\n","    input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='input')\r\n","\r\n","    # Add a text preprocessing layer to convert input into proper format for\r\n","    # BERT (see URL on the next line for more details)\r\n","    preprocess_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1'\r\n","    preprocessing_layer = hub.KerasLayer(preprocess_url)\r\n","    encoder_inputs = preprocessing_layer(input)\r\n","\r\n","    # Use BERT encoder (see URL on the next line for more details)\r\n","    encoder_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'\r\n","    encoder = hub.KerasLayer(encoder_url,\r\n","                             trainable=True)\r\n","    outputs = encoder(encoder_inputs)\r\n","\r\n","    # Add a dropout and dense layer\r\n","    net = outputs['pooled_output']\r\n","    net = tf.keras.layers.Dropout(0.1)(net)\r\n","    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\r\n","    model = tf.keras.Model(input, net)\r\n","    return model\r\n","\r\n","def train_and_predict_bert(train_x, train_y, val_x, val_y, test_x):\r\n","    \"\"\"Compiles and trains the BERT model.\r\n","\r\n","       SOURCE: This function borrows heavily from Tensorflow's \"Classify Text\r\n","       with BERT\" tutorial at the URL below:\r\n","       https://www.tensorflow.org/tutorials/text/classify_text_with_bert.\r\n","    \"\"\"\r\n","    model = get_bert_classifier()\r\n","\r\n","    # Use binary crossentropy loss function, and track binary accuracy\r\n","    # TODO: experimenting with different metrics, and testing reproducibility\r\n","    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n","    metrics = tf.metrics.BinaryAccuracy()\r\n","\r\n","    epochs = 5\r\n","    steps_per_epoch = 80\r\n","    num_train_steps = steps_per_epoch * epochs\r\n","    num_warmup_steps = int(0.1 * num_train_steps)\r\n","\r\n","    init_lr = 3e-5\r\n","    optimizer = optimization.create_optimizer(init_lr=init_lr,\r\n","                                              num_train_steps=num_train_steps,\r\n","                                              num_warmup_steps=num_warmup_steps,\r\n","                                              optimizer_type='adamw')\r\n","\r\n","    # Compile the model\r\n","    model.compile(optimizer=optimizer,\r\n","                  loss=loss,\r\n","                  metrics=metrics)\r\n","\r\n","    # Fit the model to the training data, tracking binary accuracy on the \r\n","    # validation set\r\n","    print('Fitting classifier...')\r\n","    model.fit(x=train_x, y=train_y, \r\n","              validation_data=(val_x, val_y), \r\n","              epochs=epochs)\r\n","    \r\n","    # Predict labels for the test data\r\n","    print('Making predictions...')\r\n","    predictions = model.predict(test_x)\r\n","    return predictions\r\n","\r\n","def create_output_file(predictions, filename='answer.txt'):\r\n","    \"\"\"Transform predictions into the right file format for submission.\r\n","    \"\"\"\r\n","    tweet_ids = [\"twitter_\" + str(i) for i in range(1, len(predictions) + 1)]\r\n","    predicted_labels = [\"SARCASM\" if predictions[i] > 0 else \"NOT_SARCASM\" for i in range(len(predictions))]    \r\n","    predictions_df = pd.DataFrame(data={\"id\": tweet_ids, \"label\": predicted_labels})\r\n","    predictions_df.to_csv(filename, header=False, index=False)\r\n","    print('Done. Remember to download answer.txt before session disconnects.')"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y9WvC-vgMF5N"},"source":["### 4. Run the full pipeline of reading in train and test data, training the model, and making predictions (outputting to `answer.txt`)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dycbL5DWFNnZ","executionInfo":{"status":"ok","timestamp":1607823036160,"user_tz":480,"elapsed":345112,"user":{"displayName":"Sam Squires","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqEAf4ZjxyoXjCT4o-1d19bfq9In0PRa0MDNcW__I=s64","userId":"16893370824620180660"}},"outputId":"1aca6177-3e6d-47a3-9fc6-0613fa4276d5"},"source":["# Set random seed for reproducibility\r\n","random.seed(123)\r\n","tf.random.set_seed(123)\r\n","\r\n","# Train and test file (make sure to upload these!!!)\r\n","train_file = 'train.jsonl'\r\n","test_file = 'test.jsonl'\r\n","\r\n","# For both train and test data, combine 'response' and 'context' from original\r\n","# data into one long string feature named 'feature' (not creative)\r\n","train = pd.read_json(train_file, lines=True)\r\n","train['context'] = [','.join(map(str, l)) for l in train['context']]\r\n","train['feature'] = train['response'] + ' ' + train['context']\r\n","\r\n","test = pd.read_json(test_file, lines=True)\r\n","test['context'] = [','.join(map(str, l)) for l in test['context']]\r\n","test['feature'] = test['response'] + ' ' + test['context']\r\n","\r\n","# Split training data into train and validation set (setting random seed for\r\n","# reproducibility)\r\n","train_x, val_x, train_y, val_y = train_test_split(train['feature'].values,\r\n","                                                  train['label'].values,\r\n","                                                  test_size=0.2,\r\n","                                                  random_state=42)\r\n","\r\n","# Use newly created 'feature' column (response + context) as test feature\r\n","test_x = test['feature'].values\r\n","\r\n","# Reshape all feature/label vectors for input into tf\r\n","train_x = train_x.reshape((-1, 1))\r\n","val_x = val_x.reshape((-1, 1))\r\n","test_x = test_x.reshape((-1, 1))\r\n","train_y = train_y.reshape((-1, 1))\r\n","val_y = val_y.reshape((-1, 1))\r\n","\r\n","# Represent train and validation labels as 1 (sarcasm) and 0 (not sarcasm)\r\n","get_labels = np.vectorize(lambda x: 1 if x == 'SARCASM' else 0)\r\n","train_y = get_labels(train_y)\r\n","val_y = get_labels(val_y)\r\n","\r\n","# Train our BERT model and output final prediction file for test data\r\n","predictions = train_and_predict_bert(train_x, train_y, val_x, val_y, test_x)\r\n","create_output_file(predictions, \"answer.txt\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Fitting classifier...\n","Epoch 1/5\n","125/125 [==============================] - 125s 1s/step - loss: 0.5700 - binary_accuracy: 0.6685 - val_loss: 0.4974 - val_binary_accuracy: 0.7670\n","Epoch 2/5\n","125/125 [==============================] - 127s 1s/step - loss: 0.4220 - binary_accuracy: 0.7945 - val_loss: 0.4262 - val_binary_accuracy: 0.8010\n","Epoch 3/5\n","125/125 [==============================] - 127s 1s/step - loss: 0.2941 - binary_accuracy: 0.8710 - val_loss: 0.4309 - val_binary_accuracy: 0.8060\n","Epoch 4/5\n","125/125 [==============================] - 127s 1s/step - loss: 0.2236 - binary_accuracy: 0.9097 - val_loss: 0.4353 - val_binary_accuracy: 0.8110\n","Epoch 5/5\n","125/125 [==============================] - 127s 1s/step - loss: 0.2238 - binary_accuracy: 0.9080 - val_loss: 0.4353 - val_binary_accuracy: 0.8110\n","Making predictions...\n","Done. Remember to download answer.txt before session disconnects.\n"],"name":"stdout"}]}]}